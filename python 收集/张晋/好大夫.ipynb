{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "149\n",
      "179\n",
      "209\n",
      "234\n",
      "256\n",
      "278\n",
      "296\n",
      "316\n",
      "334\n",
      "353\n",
      "366\n",
      "386\n",
      "399\n",
      "426\n",
      "450\n",
      "474\n",
      "489\n",
      "511\n",
      "537\n",
      "543\n",
      "547\n",
      "567\n",
      "586\n",
      "602\n",
      "617\n",
      "637\n",
      "654\n",
      "668\n",
      "693\n",
      "702\n",
      "728\n",
      "736\n",
      "756\n",
      "779\n",
      "783\n",
      "803\n",
      "819\n",
      "834\n",
      "851\n",
      "862\n",
      "883\n",
      "891\n",
      "913\n",
      "940\n",
      "946\n",
      "957\n",
      "987\n",
      "1016\n",
      "1046\n",
      "1075\n",
      "1086\n",
      "1089\n",
      "1092\n",
      "1096\n",
      "1102\n",
      "1105\n",
      "1109\n",
      "1112\n",
      "1117\n",
      "1122\n",
      "1124\n",
      "1127\n",
      "1128\n",
      "1150\n",
      "1176\n",
      "1187\n",
      "1214\n",
      "1217\n",
      "1238\n",
      "1256\n",
      "1262\n",
      "1290\n",
      "1318\n",
      "1323\n",
      "1331\n",
      "1361\n",
      "1387\n",
      "1406\n",
      "1410\n",
      "1434\n",
      "1461\n",
      "1487\n",
      "1500\n",
      "1512\n",
      "1524\n",
      "1553\n",
      "1575\n",
      "1602\n",
      "1616\n",
      "1622\n",
      "1636\n",
      "1652\n",
      "1682\n",
      "1711\n",
      "1732\n",
      "1762\n",
      "1765\n",
      "1771\n",
      "1778\n",
      "1781\n",
      "1789\n",
      "1803\n",
      "1832\n",
      "1855\n",
      "1884\n",
      "1902\n",
      "1913\n",
      "1923\n",
      "1933\n",
      "1953\n",
      "1982\n",
      "2007\n",
      "2035\n",
      "2065\n",
      "2076\n",
      "2084\n",
      "2091\n",
      "2102\n",
      "2106\n",
      "2112\n",
      "2118\n",
      "2138\n",
      "2167\n",
      "2185\n",
      "2212\n",
      "2232\n",
      "2243\n",
      "2249\n",
      "2254\n",
      "2259\n",
      "2264\n",
      "2270\n",
      "2277\n",
      "2282\n",
      "2308\n",
      "2333\n",
      "2363\n",
      "2375\n",
      "2378\n",
      "2384\n",
      "2389\n",
      "2395\n",
      "2399\n",
      "2406\n",
      "2413\n",
      "2441\n",
      "2458\n",
      "2488\n",
      "2501\n",
      "2507\n",
      "2512\n",
      "2523\n",
      "2532\n",
      "2538\n",
      "2544\n",
      "2562\n",
      "2578\n",
      "2603\n",
      "2628\n",
      "2632\n",
      "2641\n",
      "2648\n",
      "2652\n",
      "2658\n",
      "2666\n",
      "2689\n",
      "2704\n",
      "2734\n",
      "2741\n",
      "2746\n",
      "2751\n",
      "2756\n",
      "2762\n",
      "2765\n",
      "2767\n",
      "2773\n",
      "2775\n",
      "2792\n",
      "2811\n",
      "2841\n",
      "2850\n",
      "2852\n",
      "2854\n",
      "2856\n",
      "2859\n",
      "2865\n",
      "2866\n",
      "2870\n",
      "2871\n",
      "2873\n",
      "2879\n",
      "2884\n",
      "2888\n",
      "2891\n",
      "2903\n",
      "2929\n",
      "2959\n",
      "2980\n",
      "2984\n",
      "2985\n",
      "2985\n",
      "2987\n",
      "2990\n",
      "2994\n",
      "2997\n",
      "3000\n",
      "3002\n",
      "3007\n",
      "3012\n",
      "3015\n",
      "3019\n",
      "3019\n",
      "3024\n",
      "3027\n",
      "3030\n",
      "3034\n",
      "3039\n",
      "3042\n",
      "3046\n",
      "3065\n",
      "3090\n",
      "3120\n",
      "3129\n",
      "3134\n",
      "3140\n",
      "3144\n",
      "3148\n",
      "3151\n",
      "3154\n",
      "3158\n",
      "3163\n",
      "3166\n",
      "3171\n",
      "3174\n",
      "3177\n",
      "3180\n",
      "3197\n",
      "3220\n",
      "3225\n",
      "3234\n",
      "3238\n",
      "3244\n",
      "3248\n",
      "3261\n",
      "3272\n",
      "3275\n",
      "3283\n",
      "3288\n",
      "3294\n",
      "3309\n",
      "3310\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3320\n",
      "3330\n",
      "3333\n",
      "3337\n",
      "3337\n",
      "3339\n",
      "3339\n",
      "3340\n",
      "3345\n",
      "3352\n",
      "3354\n",
      "3356\n",
      "3363\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3370\n",
      "3377\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3388\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from lxml import etree\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "#-------------------------------输入区域--------------------输入区域---------#\n",
    "url_of_illness = 'https://haoping.haodf.com/keshi/1008000/daifu_all.htm'    #\n",
    "illness_name = \"肾病内科\"\n",
    "#---------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "url_all_doctors = []\n",
    "csv_file_name = illness_name + \"医生主页url.csv\"\n",
    "\n",
    "# 访问网页\n",
    "def get_page(url):\n",
    "    User_Agent = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    "        'Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)'\n",
    "    ]\n",
    "\n",
    "    len_user_agent = len(User_Agent)\n",
    "    random_num = random.randint(0, len_user_agent-1)\n",
    "    user_agent = User_Agent[random_num]\n",
    "\n",
    "    response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "    response.encoding = 'cp936'\n",
    "    \n",
    "    html = response.text\n",
    "    status_code = response.status_code\n",
    "\n",
    "    #如果限制访问，执行time.sleep，否则不启用\n",
    "    count = 0\n",
    "    while status_code == 403:\n",
    "        response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "        response.encoding = 'cp936'\n",
    "\n",
    "        html = response.text\n",
    "        status_code = response.status_code\n",
    "\n",
    "        count += 1\n",
    "        time.sleep(count*0.1)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# 查找改网页下的医生（每个页面下有30个）\n",
    "def get_doctor_url_list(html):\n",
    "\tpattern = re.compile('<a href=(.*?)target=\"_blank\" class=\"blue pernet\">访问个人网站.*?</a>')\n",
    "\turl_the_page_list = re.findall(pattern,html)\n",
    "\tfor lt in url_the_page_list:\n",
    "\t\turl_valid = \"https:\" + eval(lt)\n",
    "\t\turl_all_doctors.append(url_valid)\n",
    "\n",
    "\n",
    "# 查看链接时htm还是html，统一格式，防止报错\n",
    "def check_page_structure(url):\n",
    "    if url[-1] == 'l':\n",
    "        return 5\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "# 定义查找该页面下有多少翻页子页面\n",
    "def get_pages_number(html):\n",
    "    select = etree.HTML(html)\n",
    "    number_string = select.xpath('//a[@class=\"p_text\"]//text()')[0]\n",
    "    number = re.search('\\d+',number_string).group(0)\n",
    "    number = int(number)\n",
    "    return number\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(offset,n):\n",
    "    url = url_of_illness[:-n] + '_' + str(offset) + '.htm'\n",
    "    html = get_page(url)\n",
    "    get_doctor_url_list(html)\n",
    "    print(len(url_all_doctors))\n",
    "\n",
    "\n",
    "# 定义写文件\n",
    "def write_csv(file,url_list,illness_name):\n",
    "    with open(file,'a',encoding='utf-8-sig',newline='') as csvfile:\n",
    "        fieldnames = [\"doctor_url\",\"Type_of_treatment\"]\n",
    "        writer = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in url_list:\n",
    "            writer.writerow({\"doctor_url\":i,\n",
    "            \t\t\t\t \"Type_of_treatment\":illness_name})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #爬取第一页，放入存储列表中\n",
    "    html = get_page(url_of_illness)\n",
    "    get_doctor_url_list(html)\n",
    "    print(len(url_all_doctors))\n",
    "\n",
    "    # 查看一共多少页,存储在number_of_pages中\n",
    "    number_of_pages = get_pages_number(html)\n",
    "    \n",
    "    # 检查网页的结构化，防止html和htm不一致造成的影响。\n",
    "    n = check_page_structure(url_of_illness)\n",
    "\n",
    "    # 开始offset迭代\n",
    "    for i in range(2,number_of_pages+1):\n",
    "        main(offset=i,n=n)\n",
    "\n",
    "    # 写入文件\n",
    "    write_csv(csv_file_name,url_all_doctors,illness_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neike_add=['https://haoping.haodf.com/keshi/1007000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1010000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1009000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1005000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1004000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1002000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1008000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1002000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1011000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1012000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1014000/daifu_all.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1003000/daifu_all.htm',\\\n",
    "     'https://www.haodf.com/keshi/1006000.htm',\\\n",
    "     'https://haoping.haodf.com/keshi/1001000/daifu_all.htm']\n",
    "neike_name=['神经内科','心血管内科','消化内科','内分泌科','免疫科','呼吸科',\\\n",
    "      '肾病内科','呼吸科','血液科','感染内科','过敏反应科','老年病科',\\\n",
    "      '普通内科','高压氧科']\n",
    "len(neike_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(neike_name)):\n",
    "    if neike_name[i] in ['神经内科','肾病内科']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://haoping.haodf.com/keshi/1010000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1009000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1005000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1004000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1002000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1002000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1011000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1012000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1014000/daifu_all.htm', 'https://haoping.haodf.com/keshi/1003000/daifu_all.htm', 'https://www.haodf.com/keshi/1006000.htm', 'https://haoping.haodf.com/keshi/1001000/daifu_all.htm']\n",
      "['心血管内科', '消化内科', '内分泌科', '免疫科', '呼吸科', '肾病内科', '血液科', '感染内科', '过敏反应科', '老年病科', '普通内科', '高压氧科']\n"
     ]
    }
   ],
   "source": [
    "neike_add.pop(6)\n",
    "neike_add.pop(0)\n",
    "print(neike_add)\n",
    "neike_name.pop(0)\n",
    "neike_name.pop(6)\n",
    "print(neike_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-280859bb3541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# 查看一共多少页,存储在number_of_pages中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mnumber_of_pages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pages_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# 检查网页的结构化，防止html和htm不一致造成的影响。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-280859bb3541>\u001b[0m in \u001b[0;36mget_pages_number\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_pages_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mselect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mnumber_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//a[@class=\"p_text\"]//text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\d+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumber_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from lxml import etree\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "#-------------------------------输入区域--------------------输入区域---------#\n",
    "add=['https://haoping.haodf.com/keshi/6004000/daifu_all.htm']\n",
    "\n",
    "name=['妇泌尿科']\n",
    "#---------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 访问网页\n",
    "def get_page(url):\n",
    "    User_Agent = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    "        'Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)'\n",
    "    ]\n",
    "\n",
    "    len_user_agent = len(User_Agent)\n",
    "    random_num = random.randint(0, len_user_agent-1)\n",
    "    user_agent = User_Agent[random_num]\n",
    "\n",
    "    response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "    response.encoding = 'cp936'\n",
    "    \n",
    "    html = response.text\n",
    "    status_code = response.status_code\n",
    "\n",
    "    #如果限制访问，执行time.sleep，否则不启用\n",
    "    count = 0\n",
    "    while status_code == 403:\n",
    "        response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "        response.encoding = 'cp936'\n",
    "\n",
    "        html = response.text\n",
    "        status_code = response.status_code\n",
    "\n",
    "        count += 1\n",
    "        time.sleep(count*0.1)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# 查找改网页下的医生（每个页面下有30个）\n",
    "def get_doctor_url_list(html):\n",
    "\tpattern = re.compile('<a href=(.*?)target=\"_blank\" class=\"blue pernet\">访问个人网站.*?</a>')\n",
    "\turl_the_page_list = re.findall(pattern,html)\n",
    "\tfor lt in url_the_page_list:\n",
    "\t\turl_valid = \"https:\" + eval(lt)\n",
    "\t\turl_all_doctors.append(url_valid)\n",
    "\n",
    "\n",
    "# 查看链接时htm还是html，统一格式，防止报错\n",
    "def check_page_structure(url):\n",
    "    if url[-1] == 'l':\n",
    "        return 5\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "# 定义查找该页面下有多少翻页子页面\n",
    "def get_pages_number(html):\n",
    "    select = etree.HTML(html)\n",
    "    number_string = select.xpath('//a[@class=\"p_text\"]//text()')[0]\n",
    "    number = re.search('\\d+',number_string).group(0)\n",
    "    number = int(number)\n",
    "    return number\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(offset,n):\n",
    "    url = url_of_illness[:-n] + '_' + str(offset) + '.htm'\n",
    "    html = get_page(url)\n",
    "    get_doctor_url_list(html)\n",
    "    print(len(url_all_doctors))\n",
    "\n",
    "\n",
    "# 定义写文件url_of_illness = 'https://haoping.haodf.com/keshi/1007000/daifu_all.htm'    #\n",
    "illness_name = \"神经内科\"\n",
    "def write_csv(file,url_list,illness_name):\n",
    "    with open(file,'a',encoding='utf-8-sig',newline='') as csvfile:\n",
    "        fieldnames = [\"doctor_url\",\"Type_of_treatment\"]\n",
    "        writer = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in url_list:\n",
    "            writer.writerow({\"doctor_url\":i,\n",
    "            \t\t\t\t \"Type_of_treatment\":illness_name})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(len(name)):\n",
    "        url_of_illness = add[i]   #\n",
    "        illness_name = name[i]\n",
    "\n",
    "\n",
    "        url_all_doctors = []\n",
    "        csv_file_name = illness_name + \"医生主页url.csv\"\n",
    "        \n",
    "        #爬取第一页，放入存储列表中\n",
    "        html = get_page(url_of_illness)\n",
    "        get_doctor_url_list(html)\n",
    "        print(len(url_all_doctors))\n",
    "\n",
    "        # 查看一共多少页,存储在number_of_pages中\n",
    "        number_of_pages = get_pages_number(html)\n",
    "    \n",
    "        # 检查网页的结构化，防止html和htm不一致造成的影响。\n",
    "        n = check_page_structure(url_of_illness)\n",
    "\n",
    "        # 开始offset迭代\n",
    "        for i in range(2,number_of_pages+1):\n",
    "            main(offset=i,n=n)\n",
    "        \n",
    "\n",
    "        # 写入文件\n",
    "        write_csv(csv_file_name,url_all_doctors,illness_name)\n",
    "        print(illness_name +'successful')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
