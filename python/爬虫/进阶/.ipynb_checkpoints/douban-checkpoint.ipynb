{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2961\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-1-53e00837c44a>\"\u001b[0m, line \u001b[0;32m110\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    main(i*25)\n",
      "  File \u001b[0;32m\"<ipython-input-1-53e00837c44a>\"\u001b[0m, line \u001b[0;32m102\u001b[0m, in \u001b[0;35mmain\u001b[0m\n    for item in items:\n",
      "  File \u001b[0;32m\"<ipython-input-1-53e00837c44a>\"\u001b[0m, line \u001b[0;32m79\u001b[0m, in \u001b[0;35mget_one_page\u001b[0m\n    items.append((title_lt[i],get_comments(url_lt[i])))\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-53e00837c44a>\"\u001b[1;36m, line \u001b[1;32m71\u001b[1;36m, in \u001b[1;35mget_comments\u001b[1;36m\u001b[0m\n\u001b[1;33m    comments.append(eval(_comment_))\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    我活在世上，无非想要明白些道理，遇见些有趣的事。\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import queue as Queue\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def get_page(url):\n",
    "    User_Agent = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    "        'Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)'\n",
    "    ]\n",
    "\n",
    "    count = 0\n",
    "    status_code = 403\n",
    "    while status_code != 200:\n",
    "        # 状态码不通过，sleep重新请求\n",
    "        count += 1\n",
    "        time.sleep(count*0.1)\n",
    "        \n",
    "        # 随机生成一个头部\n",
    "        len_user_agent = len(User_Agent)\n",
    "        random_num = random.randint(0, len_user_agent-1)\n",
    "        user_agent = User_Agent[random_num]\n",
    "        \n",
    "        # 请求\n",
    "        response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "        response.encoding = 'utf-8'\n",
    "        \n",
    "        # 得到html报文和状态码\n",
    "        html = response.text\n",
    "        status_code = response.status_code\n",
    "        \n",
    "    return html\n",
    "\n",
    "url_lt = []\n",
    "title_lt=[]\n",
    "def get_book_url_list(html):\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    url_list_info = soup.find_all(class_ = 'pl2')\n",
    "    pattern = re.compile('<a.*?href=(.*?)onclick=.*?title=(.*?)>.*?</a>',re.S)\n",
    "\n",
    "    for url in url_list_info:\n",
    "        url = str(url)\n",
    "        url = re.search(pattern,url)\n",
    "        url_lt.append(eval(url.group(1).strip()))\n",
    "        title_lt.append(eval(url.group(2).strip()))\n",
    "\n",
    "    return url_lt,title_lt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_comments(html_url):\n",
    "    comments=[]\n",
    "    urls=[html_url+'comments/hot?p={}'.format(str(i)) for i in range(1,5)]#多进程\n",
    "    for url in urls:\n",
    "        text=get_page(url)\n",
    "        _comments_=re.findall(r\"<span class=\\\"short\\\">(.*)</span>\",text)\n",
    "        for _comment_ in _comments_:\n",
    "            comments.append(_comment_)\n",
    "    return comments\n",
    "    \n",
    "def get_one_page(url_lt,title_lt):\n",
    "    \n",
    "    n=len(url_lt)\n",
    "    items=[]\n",
    "    for i in range(n):\n",
    "        items.append((title_lt[i],get_comments(url_lt[i])))\n",
    "    for item in items:\n",
    "        yield{\n",
    "            'title':item[0],\n",
    "            'comments':item[1]\n",
    "        }\n",
    "    return items\n",
    "    \n",
    "\n",
    "\n",
    "def write_to_file(content):\n",
    "    with open('d:/python/爬虫/文件/豆瓣图书_评论.csv','a',encoding='utf-8',newline='') as f:\n",
    "        fieldnames=['title','comments']\n",
    "        writer=csv.DictWriter(f,fieldnames=fieldnames)\n",
    "        '''writer.writeheader()'''\n",
    "        writer.writerows(content)\n",
    "'''\n",
    "content=[]\n",
    "url = 'https://book.douban.com/top250?start=25'\n",
    "html=get_page(url)\n",
    "url_lt,title_lt=get_book_url_list(html)\n",
    "items=get_one_page(url_lt,title_lt)\n",
    "for item in items:\n",
    "    content.append(eval(json.dumps(item,ensure_ascii=False)))\n",
    "print(content)\n",
    "\n",
    "\n",
    "'''\n",
    "def main(offset):\n",
    "    url = 'https://book.douban.com/top250?start=' + str(offset)\n",
    "    html=get_page(url)\n",
    "    url_lt,title_lt=get_book_url_list(html)    #得到该网页所有书的url-->html_0_lt\n",
    "    items=get_one_page(url_lt,title_lt)\n",
    "    for item in items:\n",
    "        content.append(eval(json.dumps(item,ensure_ascii=False)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    a=time.perf_counter()\n",
    "    content=[]\n",
    "    for i in range(4,11):\n",
    "        main(i*25)\n",
    "        print('page'+str(i))\n",
    "    write_to_file(content)\n",
    "    b=time.perf_counter()\n",
    "    print(b-a)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "page4\n",
    "page5\n",
    "page6\n",
    "page7\n",
    "page8\n",
    "page9\n",
    "page10\n",
    "2926.9000677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从2:59 p.m----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(offset):\n",
    "    url = 'https://book.douban.com/top250?start=' + str(offset)\n",
    "    html = get_one_page(url)\n",
    "    get_book_url_list(html)\n",
    "    print(len(url_lt))\n",
    "\n",
    "\n",
    "def write_csv(file,url_list):\n",
    "    with open(file,'a',encoding='utf-8',newline='') as csvfile:\n",
    "        fieldnames = [\"rank\",\"book_url\"]\n",
    "        writer = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(len(url_list)):\n",
    "            writer.writerow({\"rank\":i+1,\"book_url\":url_list[i]})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        main(i)\n",
    "    write_csv(\"douban_TOP250_data.csv\",url_lt)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function find_all in module bs4.element:\n",
      "\n",
      "find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      "    Extracts a list of Tag objects that match the given\n",
      "    criteria.  You can specify the name of the Tag and any\n",
      "    attributes you want the Tag to have.\n",
      "    \n",
      "    The value of a key-value pair in the 'attrs' map can be a\n",
      "    string, a list of strings, a regular expression object, or a\n",
      "    callable that takes a string and returns whether or not the\n",
      "    string matches for some custom definition of 'matches'. The\n",
      "    same is true of the tag name.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BeautifulSoup.find_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#张晋\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from lxml import etree\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "book_name_list = []\n",
    "author_list = []\n",
    "translator_list = []\n",
    "publishing_house_list = []\n",
    "publishing_time_list = []\n",
    "price_list = []\n",
    "score_list = []\n",
    "comment_list = []\n",
    "sentence_list = []\n",
    "\n",
    "def get_one_page(url):\n",
    "    try:\n",
    "        headers = {\n",
    "                \"User_Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
    "                }\n",
    "        response = requests.get(url,headers=headers,timeout = 5)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_one_page(html):\n",
    "    select = etree.HTML(html)\n",
    "    # 处理book_name\n",
    "    book_name = select.xpath('//div[@class=\"pl2\"]/a/text()')\n",
    "    for i in range(len(book_name)):\n",
    "        book_name[i] = book_name[i].strip()\n",
    "    book_name = list(filter(None, book_name))\n",
    "\n",
    "    books_info = select.xpath('//p[@class=\"pl\"]//text()')\n",
    "    score = select.xpath('//span[@class=\"rating_nums\"]//text()')\n",
    "    comment = select.xpath('//span[@class=\"pl\"]//text()')\n",
    "    message = select.xpath('//span[@class=\"inq\"]//text()')\n",
    "\n",
    "    author_sub_list = []\n",
    "    translator_sub_list = []\n",
    "    publishing_house_sub_list = []\n",
    "    publishing_time_sub_list = []\n",
    "    price_sub_list = []    \n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(len(books_info)):\n",
    "        info = str(books_info[i])\n",
    "        info = info.split('/')\n",
    "        while len(info) <= 4:\n",
    "            info.insert(1,'NA')\n",
    "        \n",
    "        author_sub_list.append(info[0].strip())\n",
    "        translator_sub_list.append(info[1].strip())\n",
    "        publishing_house_sub_list.append(info[2].strip())\n",
    "        publishing_time_sub_list.append(info[3].strip())\n",
    "        price_sub_list.append(info[4].strip())\n",
    "\n",
    "    book_name_list.extend(book_name)\n",
    "    author_list.extend(author_sub_list)\n",
    "    translator_list.extend(translator_sub_list)\n",
    "    publishing_house_list.extend(publishing_house_sub_list)\n",
    "    publishing_time_list.extend(publishing_time_sub_list)\n",
    "    price_list.extend(price_sub_list)\n",
    "    score_list.extend(score)\n",
    "    comment_list.extend(comment)\n",
    "    sentence_list.extend(message)\n",
    "\n",
    "\n",
    "def write_csv(filename):\n",
    "    with open(filename,'w+',encoding='utf-8-sig',newline='') as csvfile:\n",
    "        fieldnames = [\"rank\",\"book_name\",\"author\",\"translator\",\"publishing_house\",\"publishing_time\",\"price\",\"score\",\"comment\"]\n",
    "        writer = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(len(book_name_list)):\n",
    "            writer.writerow({\n",
    "                \"rank\":str(i+1),\n",
    "                \"book_name\":book_name_list[i],\n",
    "                \"author\":author_list[i],\n",
    "                \"translator\":translator_list[i],\n",
    "                \"publishing_house\":publishing_house_list[i],\n",
    "                \"publishing_time\":publishing_time_list[i],\n",
    "                \"price\":price_list[i],\n",
    "                \"score\":score_list[i],\n",
    "                \"comment\":comment_list[i]\n",
    "                })\n",
    "\n",
    "\n",
    "def main(offset):\n",
    "    url = 'https://book.douban.com/top250?start=' + str(offset)\n",
    "    html = get_one_page(url)\n",
    "    parse_one_page(html)\n",
    "    print(\"book_name={},author={},translator={},publishing_house={},publishing_time={},price={},score={},comment={},sentence={}\".format(len(book_name_list),len(author_list),len(translator_list),len(publishing_house_list),len(publishing_time_list),len(price_list),len(score_list),len(comment_list),len(sentence_list)))\n",
    "    write_csv(\"douban_info.csv\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        main(offset = i*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "import queue as Queue\n",
    "import random\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_page(url):\n",
    "    User_Agent = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    "        'Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)'\n",
    "    ]\n",
    "\n",
    "    count = 0\n",
    "    status_code = 403\n",
    "    while status_code != 200:\n",
    "        # 状态码不通过，sleep重新请求\n",
    "        count += 1\n",
    "        time.sleep(count*0.1)\n",
    "        \n",
    "        # 随机生成一个头部\n",
    "        len_user_agent = len(User_Agent)\n",
    "        random_num = random.randint(0, len_user_agent-1)\n",
    "        user_agent = User_Agent[random_num]\n",
    "        \n",
    "        # 请求\n",
    "        response = requests.get(url=url,headers={'User-Agent': user_agent})\n",
    "        response.encoding = 'utf-8'\n",
    "        \n",
    "        # 得到html报文和状态码\n",
    "        html = response.text\n",
    "        status_code = response.status_code\n",
    "        \n",
    "    return html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
